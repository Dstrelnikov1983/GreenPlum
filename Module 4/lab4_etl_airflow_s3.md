# –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ ‚Ññ4: ETL-–ø—Ä–æ—Ü–µ—Å—Å —Å Apache Airflow –∏ Object Storage

## –¶–µ–ª—å —Ä–∞–±–æ—Ç—ã

–°–æ–∑–¥–∞—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π ETL-–ø—Ä–æ—Ü–µ—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Yandex Object Storage –≤ GreenPlum (Yandex MPP Analytics for PostgreSQL) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Managed Service for Apache Airflow‚Ñ¢. –ù–∞—Å—Ç—Ä–æ–∏—Ç—å —Å–µ—Ç–µ–≤—É—é —Å–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–∏—Å–æ–≤ Yandex Cloud.

## –ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

**90 –º–∏–Ω—É—Ç** (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

## –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

- –ê–∫–∫–∞—É–Ω—Ç –≤ Yandex Cloud —Å –∞–∫—Ç–∏–≤–Ω—ã–º –±–∏–ª–ª–∏–Ω–≥–æ–º
- –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π Yandex Cloud CLI
- –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è Python –∏ SQL
- –ü–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π ETL
- –í—ã–ø–æ–ª–Ω–µ–Ω–Ω–∞—è –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ ‚Ññ1 (–∏–ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–ª–∞—Å—Ç–µ—Ä GreenPlum)

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ä–µ—à–µ–Ω–∏—è

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Yandex Cloud                             ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ  Object Storage  ‚îÇ      ‚îÇ  Apache Airflow   ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ  (S3)            ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  (Managed)        ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ      ‚îÇ                   ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ - sales_data.csv ‚îÇ      ‚îÇ - DAG: ETL        ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ - products.json  ‚îÇ      ‚îÇ - Scheduler       ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ - logs/          ‚îÇ      ‚îÇ - Web UI          ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                                      ‚îÇ                       ‚îÇ
‚îÇ                                      ‚îÇ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ           ‚îÇ
‚îÇ                                      ‚îÇ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è        ‚îÇ
‚îÇ                                      ‚îÇ –ó–∞–≥—Ä—É–∑–∫–∞             ‚îÇ
‚îÇ                                      ‚ñº                       ‚îÇ
‚îÇ                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ                       ‚îÇ  GreenPlum Cluster      ‚îÇ           ‚îÇ
‚îÇ                       ‚îÇ  (MPP Analytics)        ‚îÇ           ‚îÇ
‚îÇ                       ‚îÇ                         ‚îÇ           ‚îÇ
‚îÇ                       ‚îÇ  - Master               ‚îÇ           ‚îÇ
‚îÇ                       ‚îÇ  - Segments (√ó2)        ‚îÇ           ‚îÇ
‚îÇ                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  –í—Å–µ —Å–µ—Ä–≤–∏—Å—ã –≤ –æ–¥–Ω–æ–π VPC (mpp-network)                     ‚îÇ
‚îÇ  –ü–æ–¥—Å–µ—Ç—å: 10.1.0.0/24                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## –ß–∞—Å—Ç—å 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã (20 –º–∏–Ω—É—Ç)

### –®–∞–≥ 1.1: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ GreenPlum –∫–ª–∞—Å—Ç–µ—Ä–∞
yc managed-greenplum cluster list

# –ï—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ –Ω–µ—Ç, —Å–æ–∑–¥–∞–π—Ç–µ –µ–≥–æ (–∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑ –õ–∞–±.—Ä–∞–±–æ—Ç—ã ‚Ññ1)
# –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–ª–∞—Å—Ç–µ—Ä

# –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –∏–º—è/ID –∫–ª–∞—Å—Ç–µ—Ä–∞
GP_CLUSTER_NAME="lab-gp-cluster"
```

### –®–∞–≥ 1.2: –°–æ–∑–¥–∞–Ω–∏–µ Object Storage (S3)

```bash
# –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∫–µ—Ç–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
BUCKET_NAME="mpp-etl-data-$(date +%s)"

yc storage bucket create \
  --name $BUCKET_NAME \
  --default-storage-class standard \
  --max-size 10737418240

echo "Bucket created: $BUCKET_NAME"

# –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –∏–º—è –±–∞–∫–µ—Ç–∞ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
echo "export BUCKET_NAME=$BUCKET_NAME" >> ~/.bashrc
```

### –®–∞–≥ 1.3: –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–ª—é—á–∞ –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ S3

```bash
# –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞ –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ S3
yc iam service-account create \
  --name s3-airflow-sa \
  --description "Service account for Airflow S3 access"

# –ü–æ–ª—É—á–µ–Ω–∏–µ ID —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞
SA_ID=$(yc iam service-account get s3-airflow-sa --format json | jq -r '.id')

# –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Ä–æ–ª–∏ storage.editor
yc resource-manager folder add-access-binding $(yc config get folder-id) \
  --role storage.editor \
  --subject serviceAccount:$SA_ID

# –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–ª—é—á–∞ –¥–æ—Å—Ç—É–ø–∞
yc iam access-key create \
  --service-account-name s3-airflow-sa \
  --description "Access key for S3" \
  --format json > s3_credentials.json

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ credentials
AWS_ACCESS_KEY_ID=$(jq -r '.access_key.key_id' s3_credentials.json)
AWS_SECRET_ACCESS_KEY=$(jq -r '.secret' s3_credentials.json)

echo "AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID"
echo "AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY"

# –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ —ç—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏—è - –æ–Ω–∏ –ø–æ–Ω–∞–¥–æ–±—è—Ç—Å—è –¥–ª—è Airflow
```

### –®–∞–≥ 1.4: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Ç–µ–≤–æ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–µ—Ç–∏ (–µ—Å–ª–∏ —Å–æ–∑–¥–∞–≤–∞–ª–∏ –≤ –õ–∞–±.—Ä–∞–±–æ—Ç–µ ‚Ññ1)
yc vpc network list

# –ï—Å–ª–∏ —Å–µ—Ç–∏ –Ω–µ—Ç, —Å–æ–∑–¥–∞–π—Ç–µ –Ω–æ–≤—É—é
yc vpc network create \
  --name mpp-network \
  --description "Network for MPP, Airflow, and S3"

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–¥—Å–µ—Ç–∏ –¥–ª—è Airflow (–µ—Å–ª–∏ –µ—ë –Ω–µ—Ç)
yc vpc subnet create \
  --name airflow-subnet-a \
  --network-name mpp-network \
  --zone ru-central1-a \
  --range 10.1.1.0/24

# –ü–æ–ª—É—á–µ–Ω–∏–µ ID –ø–æ–¥—Å–µ—Ç–∏
AIRFLOW_SUBNET_ID=$(yc vpc subnet get airflow-subnet-a --format json | jq -r '.id')
echo "Airflow Subnet ID: $AIRFLOW_SUBNET_ID"
```

### –®–∞–≥ 1.5: –°–æ–∑–¥–∞–Ω–∏–µ Security Group –¥–ª—è Airflow

```bash
# –°–æ–∑–¥–∞–Ω–∏–µ security group –¥–ª—è Airflow
yc vpc security-group create \
  --name airflow-sg \
  --network-name mpp-network \
  --description "Security group for Airflow cluster"

# –ü–æ–ª—É—á–µ–Ω–∏–µ ID security group
SG_ID=$(yc vpc security-group get airflow-sg --format json | jq -r '.id')

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª
# 1. –†–∞–∑—Ä–µ—à–∏—Ç—å –∏—Å—Ö–æ–¥—è—â–∏–π —Ç—Ä–∞—Ñ–∏–∫ (–¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ S3 –∏ GreenPlum)
yc vpc security-group update-rules $SG_ID \
  --add-rule "direction=egress,port=any,protocol=any,v4-cidrs=[0.0.0.0/0]"

# 2. –†–∞–∑—Ä–µ—à–∏—Ç—å –≤—Ö–æ–¥—è—â–∏–π —Ç—Ä–∞—Ñ–∏–∫ –Ω–∞ Web UI Airflow (–ø–æ—Ä—Ç 8080)
yc vpc security-group update-rules $SG_ID \
  --add-rule "direction=ingress,port=8080,protocol=tcp,v4-cidrs=[0.0.0.0/0]"

# 3. –†–∞–∑—Ä–µ—à–∏—Ç—å –≤–Ω—É—Ç—Ä–∏—Å–µ—Ç–µ–≤–æ–π —Ç—Ä–∞—Ñ–∏–∫
yc vpc security-group update-rules $SG_ID \
  --add-rule "direction=ingress,port=any,protocol=any,v4-cidrs=[10.1.0.0/16]"

echo "Security group configured: $SG_ID"
```

**–í–∞–∂–Ω–æ –æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏:**
- –ü—Ä–∞–≤–∏–ª–æ –¥–ª—è –ø–æ—Ä—Ç–∞ 8080 –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –¥–æ—Å—Ç—É–ø –∫ Web UI –∏–∑ –ª—é–±–æ–≥–æ –º–µ—Å—Ç–∞. –í production —Å–ª–µ–¥—É–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ IP –∞–¥—Ä–µ—Å–∞–º–∏
- Internal —Ç—Ä–∞—Ñ–∏–∫ –º–µ–∂–¥—É —Å–µ—Ä–≤–∏—Å–∞–º–∏ –∑–∞—â–∏—â–µ–Ω —á–µ—Ä–µ–∑ Security Groups
- –í—Å–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å GreenPlum –∏—Å–ø–æ–ª—å–∑—É—é—Ç SSL (sslmode=require)

## –ß–∞—Å—Ç—å 2: –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ Apache Airflow (20 –º–∏–Ω—É—Ç)

### –®–∞–≥ 2.1: –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞ –¥–ª—è Airflow

```bash
# –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞ –¥–ª—è Airflow
yc iam service-account create \
  --name airflow-sa \
  --description "Service account for Apache Airflow"

# –ü–æ–ª—É—á–µ–Ω–∏–µ ID —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞
AIRFLOW_SA_ID=$(yc iam service-account get airflow-sa --format json | jq -r '.id')

# –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ä–æ–ª–µ–π
yc resource-manager folder add-access-binding $(yc config get folder-id) \
  --role managed-airflow.integrationProvider \
  --subject serviceAccount:$AIRFLOW_SA_ID

yc resource-manager folder add-access-binding $(yc config get folder-id) \
  --role vpc.user \
  --subject serviceAccount:$AIRFLOW_SA_ID

yc resource-manager folder add-access-binding $(yc config get folder-id) \
  --role storage.viewer \
  --subject serviceAccount:$AIRFLOW_SA_ID
```

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –†–æ–ª–∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç:
- `managed-airflow.integrationProvider` - –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –¥—Ä—É–≥–∏–º–∏ —Å–µ—Ä–≤–∏—Å–∞–º–∏
- `vpc.user` - –¥–æ—Å—Ç—É–ø –∫ —Å–µ—Ç–µ–≤—ã–º —Ä–µ—Å—É—Ä—Å–∞–º
- `storage.viewer` - —á—Ç–µ–Ω–∏–µ –∏–∑ Object Storage

### –®–∞–≥ 2.2: –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ Airflow

```bash
# –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ Apache Airflow
yc airflow cluster create \
  --name etl-airflow-cluster \
  --service-account-id $AIRFLOW_SA_ID \
  --subnet-id $AIRFLOW_SUBNET_ID \
  --security-group-ids $SG_ID \
  --dags-bucket $BUCKET_NAME \
  --webserver-resource-preset s2.micro \
  --scheduler-resource-preset s2.micro \
  --worker-resource-preset s2.micro \
  --min-worker-count 1 \
  --max-worker-count 3 \
  --triggerer-resource-preset s2.micro \
  --triggerer-count 1 \
  --admin-password "StrongPassword123!" \
  --version 2.2.4

echo "Airflow cluster creation started. This will take 10-15 minutes..."
echo "You can check the status with: yc airflow cluster get etl-airflow-cluster"
```

**–û–∂–∏–¥–∞–Ω–∏–µ:** –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ –∑–∞–π–º–µ—Ç –æ–∫–æ–ª–æ 10-15 –º–∏–Ω—É—Ç. –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ —Å—Ç–∞—Ç—É—Å:

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ —Å–æ–∑–¥–∞–Ω–∏—è
watch -n 30 "yc airflow cluster get etl-airflow-cluster --format json | jq -r '.status'"

# –ö–æ–≥–¥–∞ —Å—Ç–∞—Ç—É—Å —Å—Ç–∞–Ω–µ—Ç "RUNNING", –ø–æ–ª—É—á–∏—Ç–µ URL Web UI
yc airflow cluster get etl-airflow-cluster --format json | jq -r '.webserver.url'
```

### –®–∞–≥ 2.3: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ GreenPlum –≤ Airflow

–ü–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞, –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ Connection –≤ Airflow Web UI:

1. –û—Ç–∫—Ä–æ–π—Ç–µ Web UI Airflow (URL –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —à–∞–≥–∞)
2. –í–æ–π–¥–∏—Ç–µ —Å credentials:
   - Username: `admin`
   - Password: `StrongPassword123!`
3. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ **Admin ‚Üí Connections**
4. –ù–∞–∂–º–∏—Ç–µ **+ Add a new connection**
5. –ó–∞–ø–æ–ª–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:
   - **Connection Id**: `greenplum_default`
   - **Connection Type**: `Postgres`
   - **Host**: `<FQDN –≤–∞—à–µ–≥–æ GreenPlum Master>` (–ø–æ–ª—É—á–∏—Ç–µ —á–µ—Ä–µ–∑ `yc managed-greenplum hosts list lab-gp-cluster`)
   - **Schema**: `postgres`
   - **Login**: `admin`
   - **Password**: `<–≤–∞—à –ø–∞—Ä–æ–ª—å –æ—Ç GreenPlum>`
   - **Port**: `6432`
   - **Extra**: `{"sslmode": "require"}`
6. –ù–∞–∂–º–∏—Ç–µ **Test** –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
7. –ù–∞–∂–º–∏—Ç–µ **Save**

**–ü–æ–ª—É—á–µ–Ω–∏–µ FQDN GreenPlum Master:**
```bash
yc managed-greenplum hosts list $GP_CLUSTER_NAME \
  --format json | jq -r '.[] | select(.type == "MASTER") | .name'
```

### –®–∞–≥ 2.4: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ S3 –≤ Airflow

–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ —Å–æ–∑–¥–∞–π—Ç–µ Connection –¥–ª—è S3:

1. –í Airflow Web UI: **Admin ‚Üí Connections ‚Üí + Add**
2. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
   - **Connection Id**: `s3_default`
   - **Connection Type**: `Amazon S3`
   - **Extra**: 
   ```json
   {
     "aws_access_key_id": "<–≤–∞—à AWS_ACCESS_KEY_ID>",
     "aws_secret_access_key": "<–≤–∞—à AWS_SECRET_ACCESS_KEY>",
     "endpoint_url": "https://storage.yandexcloud.net",
     "region_name": "ru-central1"
   }
   ```
3. –ù–∞–∂–º–∏—Ç–µ **Test** –∏ **Save**

## –ß–∞—Å—Ç—å 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (10 –º–∏–Ω—É—Ç)

### –®–∞–≥ 3.1: –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö

–°–æ–∑–¥–∞–¥–∏–º –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏:

```bash
# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
mkdir -p ~/airflow_lab_data
cd ~/airflow_lab_data

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∞–π–ª–∞ sales_data.csv
cat > sales_data.csv << 'EOF'
order_id,customer_id,product_id,quantity,price,order_date,region
1001,501,2001,2,199.99,2025-01-15,North
1002,502,2003,1,89.99,2025-01-15,South
1003,503,2002,5,49.99,2025-01-16,East
1004,501,2001,1,199.99,2025-01-16,North
1005,504,2004,3,129.99,2025-01-17,West
1006,505,2002,2,49.99,2025-01-17,East
1007,502,2003,4,89.99,2025-01-18,South
1008,506,2005,1,299.99,2025-01-18,North
1009,507,2001,2,199.99,2025-01-19,West
1010,503,2004,1,129.99,2025-01-19,East
EOF

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∞–π–ª–∞ products.json
cat > products.json << 'EOF'
[
  {"product_id": 2001, "name": "Laptop Pro", "category": "Electronics", "stock": 50},
  {"product_id": 2002, "name": "Wireless Mouse", "category": "Accessories", "stock": 200},
  {"product_id": 2003, "name": "USB-C Cable", "category": "Accessories", "stock": 150},
  {"product_id": 2004, "name": "Mechanical Keyboard", "category": "Peripherals", "stock": 80},
  {"product_id": 2005, "name": "4K Monitor", "category": "Electronics", "stock": 30}
]
EOF

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∞–π–ª–∞ customers.csv
cat > customers.csv << 'EOF'
customer_id,name,email,registration_date,country
501,John Doe,john.doe@email.com,2024-01-10,USA
502,Jane Smith,jane.smith@email.com,2024-02-15,Canada
503,Bob Johnson,bob.j@email.com,2024-03-20,UK
504,Alice Williams,alice.w@email.com,2024-04-12,Germany
505,Charlie Brown,charlie.b@email.com,2024-05-08,France
506,Diana Prince,diana.p@email.com,2024-06-22,USA
507,Eve Adams,eve.a@email.com,2024-07-30,Canada
EOF

echo "Test data files created in ~/airflow_lab_data/"
```

### –®–∞–≥ 3.2: –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –≤ Object Storage

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ AWS CLI (–µ—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)
pip3 install awscli --user

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è AWS CLI –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Yandex Object Storage
aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID
aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY
aws configure set region ru-central1

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –≤ –±–∞–∫–µ—Ç
aws s3 cp sales_data.csv s3://$BUCKET_NAME/input/sales/ \
  --endpoint-url=https://storage.yandexcloud.net

aws s3 cp products.json s3://$BUCKET_NAME/input/products/ \
  --endpoint-url=https://storage.yandexcloud.net

aws s3 cp customers.csv s3://$BUCKET_NAME/input/customers/ \
  --endpoint-url=https://storage.yandexcloud.net

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
aws s3 ls s3://$BUCKET_NAME/input/ --recursive \
  --endpoint-url=https://storage.yandexcloud.net

echo "Files uploaded to S3 bucket: $BUCKET_NAME"
```

## –ß–∞—Å—Ç—å 4: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ö–µ–º—ã –¥–∞–Ω–Ω—ã—Ö –≤ GreenPlum (10 –º–∏–Ω—É—Ç)

### –®–∞–≥ 4.1: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ GreenPlum

```bash
# –ü–æ–ª—É—á–µ–Ω–∏–µ FQDN Master —Ö–æ—Å—Ç–∞
GP_MASTER_FQDN=$(yc managed-greenplum hosts list $GP_CLUSTER_NAME \
  --format json | jq -r '.[] | select(.type == "MASTER") | .name')

echo "GreenPlum Master FQDN: $GP_MASTER_FQDN"

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ psql
psql "host=$GP_MASTER_FQDN port=6432 dbname=postgres user=admin sslmode=require"
```

### –®–∞–≥ 4.2: –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ö–µ–º—ã –∏ —Ç–∞–±–ª–∏—Ü

–í—ã–ø–æ–ª–Ω–∏—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ SQL –∫–æ–º–∞–Ω–¥—ã –≤ psql:

```sql
-- –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ö–µ–º—ã –¥–ª—è ETL –¥–∞–Ω–Ω—ã—Ö
CREATE SCHEMA IF NOT EXISTS etl_data;

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –ø—Ä–æ–¥–∞–∂
CREATE TABLE etl_data.sales (
    order_id INTEGER,
    customer_id INTEGER,
    product_id INTEGER,
    quantity INTEGER,
    price NUMERIC(10,2),
    order_date DATE,
    region VARCHAR(50),
    load_timestamp TIMESTAMP DEFAULT now()
) DISTRIBUTED BY (customer_id)
PARTITION BY RANGE (order_date)
(
    START (DATE '2025-01-01') INCLUSIVE
    END (DATE '2025-12-31') EXCLUSIVE
    EVERY (INTERVAL '1 month')
);

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤
CREATE TABLE etl_data.products (
    product_id INTEGER PRIMARY KEY,
    name VARCHAR(200),
    category VARCHAR(100),
    stock INTEGER,
    load_timestamp TIMESTAMP DEFAULT now()
) DISTRIBUTED REPLICATED;

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–æ–≤
CREATE TABLE etl_data.customers (
    customer_id INTEGER PRIMARY KEY,
    name VARCHAR(200),
    email VARCHAR(200),
    registration_date DATE,
    country VARCHAR(100),
    load_timestamp TIMESTAMP DEFAULT now()
) DISTRIBUTED BY (customer_id);

-- Staging —Ç–∞–±–ª–∏—Ü–∞ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è
CREATE TABLE etl_data.sales_staging (
    order_id INTEGER,
    customer_id INTEGER,
    product_id INTEGER,
    quantity INTEGER,
    price NUMERIC(10,2),
    order_date DATE,
    region VARCHAR(50)
) DISTRIBUTED RANDOMLY;

-- –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü
\dt etl_data.*

-- –í—ã—Ö–æ–¥
\q
```

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ –æ –¥–∏–∑–∞–π–Ω–µ —Ç–∞–±–ª–∏—Ü:**
- `sales` - –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∞ –ø–æ –¥–∞—Ç–∞–º –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
- `products` - REPLICATED, —Ç.–∫. —ç—Ç–æ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ (–º–∞–ª–µ–Ω—å–∫–∞—è —Ç–∞–±–ª–∏—Ü–∞)
- `customers` - DISTRIBUTED BY customer_id –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ JOIN —Å sales
- `sales_staging` - RANDOM –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≤—Å—Ç–∞–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö

## –ß–∞—Å—Ç—å 5: –°–æ–∑–¥–∞–Ω–∏–µ DAG –¥–ª—è ETL-–ø—Ä–æ—Ü–µ—Å—Å–∞ (20 –º–∏–Ω—É—Ç)

### –®–∞–≥ 5.1: –°–æ–∑–¥–∞–Ω–∏–µ Python DAG —Ñ–∞–π–ª–∞

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª —Å DAG –ª–æ–∫–∞–ª—å–Ω–æ. –≠—Ç–æ—Ç DAG –±—É–¥–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø–æ–ª–Ω—ã–π ETL pipeline:

```bash
mkdir -p ~/airflow_dags
cd ~/airflow_dags
```

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `etl_s3_to_greenplum.py` —Å–æ —Å–ª–µ–¥—É—é—â–∏–º —Å–æ–¥–µ—Ä–∂–∏–º—ã–º:

```python
"""
ETL DAG: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ S3 –≤ GreenPlum
–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: Extract ‚Üí Transform ‚Üí Load
"""
from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.operators.python import PythonOperator

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
default_args = {
    'owner': 'data-engineer',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ DAG
dag = DAG(
    'etl_s3_to_greenplum',
    default_args=default_args,
    description='ETL process: S3 ‚Üí GreenPlum',
    schedule_interval='@daily',
    catchup=False,
    tags=['etl', 'greenplum', 's3'],
)

# –ó–∞–¥–∞—á–∞ 1: –û—á–∏—Å—Ç–∫–∞ staging —Ç–∞–±–ª–∏—Ü—ã
truncate_staging = PostgresOperator(
    task_id='truncate_staging_table',
    postgres_conn_id='greenplum_default',
    sql="""
        TRUNCATE TABLE etl_data.sales_staging;
    """,
    dag=dag,
)

# –ó–∞–¥–∞—á–∞ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ S3 –≤ staging
def load_sales_from_s3(**context):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ CSV —Ñ–∞–π–ª–∞ –∏–∑ S3 –≤ staging —Ç–∞–±–ª–∏—Ü—É
    """
    from airflow.providers.amazon.aws.hooks.s3 import S3Hook
    from airflow.providers.postgres.hooks.postgres import PostgresHook
    import csv
    from io import StringIO
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ S3
    s3_hook = S3Hook(aws_conn_id='s3_default')
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ –±–∞–∫–µ—Ç–∞
    bucket_name = context['params']['bucket_name']
    s3_key = 'input/sales/sales_data.csv'
    
    # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –∏–∑ S3
    file_content = s3_hook.read_key(key=s3_key, bucket_name=bucket_name)
    
    # –ü–∞—Ä—Å–∏–Ω–≥ CSV
    csv_reader = csv.DictReader(StringIO(file_content))
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ GreenPlum
    pg_hook = PostgresHook(postgres_conn_id='greenplum_default')
    conn = pg_hook.get_conn()
    cursor = conn.cursor()
    
    # –í—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    insert_count = 0
    for row in csv_reader:
        cursor.execute("""
            INSERT INTO etl_data.sales_staging 
            (order_id, customer_id, product_id, quantity, price, order_date, region)
            VALUES (%s, %s, %s, %s, %s, %s, %s)
        """, (
            int(row['order_id']),
            int(row['customer_id']),
            int(row['product_id']),
            int(row['quantity']),
            float(row['price']),
            row['order_date'],
            row['region']
        ))
        insert_count += 1
    
    conn.commit()
    cursor.close()
    conn.close()
    
    print(f"Loaded {insert_count} rows into staging table")
    return insert_count

load_sales_staging = PythonOperator(
    task_id='load_sales_from_s3',
    python_callable=load_sales_from_s3,
    params={'bucket_name': '{{ var.value.bucket_name }}'},
    dag=dag,
)

# –ó–∞–¥–∞—á–∞ 3: –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏–∑ JSON
def load_products_from_s3(**context):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ JSON —Ñ–∞–π–ª–∞ —Å –ø—Ä–æ–¥—É–∫—Ç–∞–º–∏ –∏–∑ S3
    """
    from airflow.providers.amazon.aws.hooks.s3 import S3Hook
    from airflow.providers.postgres.hooks.postgres import PostgresHook
    import json
    
    s3_hook = S3Hook(aws_conn_id='s3_default')
    bucket_name = context['params']['bucket_name']
    s3_key = 'input/products/products.json'
    
    # –ß—Ç–µ–Ω–∏–µ JSON –∏–∑ S3
    file_content = s3_hook.read_key(key=s3_key, bucket_name=bucket_name)
    products = json.loads(file_content)
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ GreenPlum
    pg_hook = PostgresHook(postgres_conn_id='greenplum_default')
    conn = pg_hook.get_conn()
    cursor = conn.cursor()
    
    # Upsert –ø—Ä–æ–¥—É–∫—Ç–æ–≤
    for product in products:
        cursor.execute("""
            INSERT INTO etl_data.products (product_id, name, category, stock)
            VALUES (%s, %s, %s, %s)
            ON CONFLICT (product_id) DO UPDATE SET
                name = EXCLUDED.name,
                category = EXCLUDED.category,
                stock = EXCLUDED.stock,
                load_timestamp = now()
        """, (
            product['product_id'],
            product['name'],
            product['category'],
            product['stock']
        ))
    
    conn.commit()
    cursor.close()
    conn.close()
    
    print(f"Loaded {len(products)} products")
    return len(products)

load_products = PythonOperator(
    task_id='load_products_from_s3',
    python_callable=load_products_from_s3,
    params={'bucket_name': '{{ var.value.bucket_name }}'},
    dag=dag,
)

# –ó–∞–¥–∞—á–∞ 4: –ó–∞–≥—Ä—É–∑–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤
def load_customers_from_s3(**context):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ CSV —Ñ–∞–π–ª–∞ —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏ –∏–∑ S3
    """
    from airflow.providers.amazon.aws.hooks.s3 import S3Hook
    from airflow.providers.postgres.hooks.postgres import PostgresHook
    import csv
    from io import StringIO
    
    s3_hook = S3Hook(aws_conn_id='s3_default')
    bucket_name = context['params']['bucket_name']
    s3_key = 'input/customers/customers.csv'
    
    file_content = s3_hook.read_key(key=s3_key, bucket_name=bucket_name)
    csv_reader = csv.DictReader(StringIO(file_content))
    
    pg_hook = PostgresHook(postgres_conn_id='greenplum_default')
    conn = pg_hook.get_conn()
    cursor = conn.cursor()
    
    insert_count = 0
    for row in csv_reader:
        cursor.execute("""
            INSERT INTO etl_data.customers 
            (customer_id, name, email, registration_date, country)
            VALUES (%s, %s, %s, %s, %s)
            ON CONFLICT (customer_id) DO UPDATE SET
                name = EXCLUDED.name,
                email = EXCLUDED.email,
                country = EXCLUDED.country,
                load_timestamp = now()
        """, (
            int(row['customer_id']),
            row['name'],
            row['email'],
            row['registration_date'],
            row['country']
        ))
        insert_count += 1
    
    conn.commit()
    cursor.close()
    conn.close()
    
    print(f"Loaded {insert_count} customers")
    return insert_count

load_customers = PythonOperator(
    task_id='load_customers_from_s3',
    python_callable=load_customers_from_s3,
    params={'bucket_name': '{{ var.value.bucket_name }}'},
    dag=dag,
)

# –ó–∞–¥–∞—á–∞ 5: –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
validate_staging_data = PostgresOperator(
    task_id='validate_staging_data',
    postgres_conn_id='greenplum_default',
    sql="""
        DO $$
        DECLARE
            null_count INTEGER;
        BEGIN
            SELECT COUNT(*) INTO null_count
            FROM etl_data.sales_staging
            WHERE order_id IS NULL 
               OR customer_id IS NULL 
               OR product_id IS NULL;
            
            IF null_count > 0 THEN
                RAISE EXCEPTION 'Found % rows with NULL values', null_count;
            END IF;
            
            RAISE NOTICE 'Data validation passed';
        END $$;
    """,
    dag=dag,
)

# –ó–∞–¥–∞—á–∞ 6: –ó–∞–≥—Ä—É–∑–∫–∞ –≤ production
load_to_production = PostgresOperator(
    task_id='load_staging_to_production',
    postgres_conn_id='greenplum_default',
    sql="""
        INSERT INTO etl_data.sales 
        (order_id, customer_id, product_id, quantity, price, order_date, region)
        SELECT 
            order_id, customer_id, product_id, quantity, 
            price, order_date, region
        FROM etl_data.sales_staging
        ON CONFLICT DO NOTHING;
    """,
    dag=dag,
)

# –ó–∞–¥–∞—á–∞ 7: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
update_statistics = PostgresOperator(
    task_id='update_table_statistics',
    postgres_conn_id='greenplum_default',
    sql="""
        ANALYZE etl_data.sales;
        ANALYZE etl_data.products;
        ANALYZE etl_data.customers;
    """,
    dag=dag,
)

# –ó–∞–¥–∞—á–∞ 8: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
def generate_load_report(**context):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö ETL
    """
    from airflow.providers.postgres.hooks.postgres import PostgresHook
    
    pg_hook = PostgresHook(postgres_conn_id='greenplum_default')
    
    stats_query = """
        SELECT 
            'Sales' as table_name,
            COUNT(*) as total_rows,
            COUNT(*) FILTER (WHERE load_timestamp::date = CURRENT_DATE) as loaded_today
        FROM etl_data.sales
        UNION ALL
        SELECT 
            'Products' as table_name,
            COUNT(*) as total_rows,
            COUNT(*) FILTER (WHERE load_timestamp::date = CURRENT_DATE) as loaded_today
        FROM etl_data.products
        UNION ALL
        SELECT 
            'Customers' as table_name,
            COUNT(*) as total_rows,
            COUNT(*) FILTER (WHERE load_timestamp::date = CURRENT_DATE) as loaded_today
        FROM etl_data.customers;
    """
    
    results = pg_hook.get_records(stats_query)
    
    print("=" * 60)
    print("ETL LOAD REPORT")
    print("=" * 60)
    for row in results:
        print(f"{row[0]:<15} | Total: {row[1]:>6} | Loaded Today: {row[2]:>6}")
    print("=" * 60)
    
    return results

generate_report = PythonOperator(
    task_id='generate_load_report',
    python_callable=generate_load_report,
    dag=dag,
)

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á
truncate_staging >> load_sales_staging >> validate_staging_data >> load_to_production
[load_products, load_customers] >> load_to_production
load_to_production >> update_statistics >> generate_report
```

–°–æ—Ö—Ä–∞–Ω–∏—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –≤ —Ñ–∞–π–ª.

### –®–∞–≥ 5.2: –ó–∞–≥—Ä—É–∑–∫–∞ DAG –≤ Object Storage

```bash
# –ó–∞–≥—Ä—É–∑–∫–∞ DAG —Ñ–∞–π–ª–∞ –≤ –±–∞–∫–µ—Ç (–≤ –ø–∞–ø–∫—É dags)
aws s3 cp etl_s3_to_greenplum.py s3://$BUCKET_NAME/dags/ \
  --endpoint-url=https://storage.yandexcloud.net

# –ü—Ä–æ–≤–µ—Ä–∫–∞
aws s3 ls s3://$BUCKET_NAME/dags/ \
  --endpoint-url=https://storage.yandexcloud.net

echo "DAG uploaded to S3. Airflow will automatically sync it in ~1 minute."
```

### –®–∞–≥ 5.3: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –≤ Airflow

1. –û—Ç–∫—Ä–æ–π—Ç–µ Airflow Web UI
2. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ **Admin ‚Üí Variables**
3. –ù–∞–∂–º–∏—Ç–µ **+ Add a new record**
4. –ó–∞–ø–æ–ª–Ω–∏—Ç–µ:
   - **Key**: `bucket_name`
   - **Val**: `<–≤–∞—à BUCKET_NAME>`
5. –ù–∞–∂–º–∏—Ç–µ **Save**

## –ß–∞—Å—Ç—å 6: –ó–∞–ø—É—Å–∫ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ ETL (10 –º–∏–Ω—É—Ç)

### –®–∞–≥ 6.1: –ê–∫—Ç–∏–≤–∞—Ü–∏—è –∏ –∑–∞–ø—É—Å–∫ DAG

1. –í Airflow Web UI –ø–µ—Ä–µ–π–¥–∏—Ç–µ –≤ **DAGs**
2. –ù–∞–π–¥–∏—Ç–µ DAG `etl_s3_to_greenplum` (–ø–æ–¥–æ–∂–¥–∏—Ç–µ ~1 –º–∏–Ω—É—Ç—É –µ—Å–ª–∏ –Ω–µ –≤–∏–¥–∏—Ç–µ)
3. –í–∫–ª—é—á–∏—Ç–µ DAG (–ø–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å —Å–ª–µ–≤–∞)
4. –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É **Trigger DAG** (‚ñ∂Ô∏è)
5. –ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ –∑–∞–ø—É—Å–∫

### –®–∞–≥ 6.2: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

1. –ö–ª–∏–∫–Ω–∏—Ç–µ –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ DAG –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –≤–∏–¥–∞
2. –í—ã–±–µ—Ä–∏—Ç–µ –∑–∞–ø—É—â–µ–Ω–Ω—ã–π run
3. –ü—Ä–æ—Å–º–æ—Ç—Ä–∏—Ç–µ **Graph View** –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
4. –ö–ª–∏–∫–∞–π—Ç–µ –Ω–∞ –∑–∞–¥–∞—á–∏ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –ª–æ–≥–æ–≤

**–û–∂–∏–¥–∞–µ–º–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ:**
- –í—Å–µ –∑–∞–¥–∞—á–∏ –¥–æ–ª–∂–Ω—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å—Å—è —É—Å–ø–µ—à–Ω–æ (–∑–µ–ª–µ–Ω—ã–π —Ü–≤–µ—Ç)
- –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: ~2-5 –º–∏–Ω—É—Ç

### –®–∞–≥ 6.3: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ GreenPlum

```bash
# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ GreenPlum
psql "host=$GP_MASTER_FQDN port=6432 dbname=postgres user=admin sslmode=require"
```

–í—ã–ø–æ–ª–Ω–∏—Ç–µ –ø—Ä–æ–≤–µ—Ä–æ—á–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã:

```sql
-- –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –ø—Ä–æ–¥–∞–∂
SELECT 
    order_date,
    COUNT(*) as order_count,
    SUM(quantity * price) as total_revenue
FROM etl_data.sales
GROUP BY order_date
ORDER BY order_date;

-- –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–≤
SELECT * FROM etl_data.products ORDER BY product_id;

-- –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤
SELECT * FROM etl_data.customers ORDER BY customer_id;

-- –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
SELECT 
    'Sales' as table_name, COUNT(*) as row_count FROM etl_data.sales
UNION ALL
SELECT 'Products', COUNT(*) FROM etl_data.products
UNION ALL
SELECT 'Customers', COUNT(*) FROM etl_data.customers;

-- –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å —Å JOIN
SELECT 
    c.name as customer_name,
    c.country,
    COUNT(s.order_id) as total_orders,
    SUM(s.quantity * s.price) as total_spent
FROM etl_data.sales s
JOIN etl_data.customers c ON s.customer_id = c.customer_id
GROUP BY c.name, c.country
ORDER BY total_spent DESC;

-- –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
SELECT 
    tablename,
    partitiontablename,
    partitionrangestart,
    partitionrangeend
FROM pg_partitions
WHERE tablename = 'sales'
ORDER BY partitionrangestart;
```

**–û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:**
- –í —Ç–∞–±–ª–∏—Ü–µ `sales` –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 10 –∑–∞–ø–∏—Å–µ–π
- –í —Ç–∞–±–ª–∏—Ü–µ `products` - 5 –∑–∞–ø–∏—Å–µ–π
- –í —Ç–∞–±–ª–∏—Ü–µ `customers` - 7 –∑–∞–ø–∏—Å–µ–π
- –î–∞–Ω–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –ø–æ –ø–∞—Ä—Ç–∏—Ü–∏—è–º

## –ß–∞—Å—Ç—å 7: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–¥–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

### –ó–∞–¥–∞–Ω–∏–µ 7.1: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏

–ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–π—Ç–µ DAG –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:

```python
# –ò–¥–µ—è: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ XCom –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π –¥–∞—Ç—ã
# –§–∏–ª—å—Ç—Ä—É–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –≤ S3 –ø–æ –¥–∞—Ç–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞
# –ó–∞–≥—Ä—É–∂–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω–∏–≤—à–∏–µ—Å—è —Ñ–∞–π–ª—ã
```

### –ó–∞–¥–∞–Ω–∏–µ 7.2: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–ª–µ—Ä—Ç–∏–Ω–≥–∞

–î–æ–±–∞–≤—å—Ç–µ email —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö:

```python
default_args = {
    ...
    'email': ['your-email@example.com'],
    'email_on_failure': True,
    'email_on_retry': False,
}
```

### –ó–∞–¥–∞–Ω–∏–µ 7.3: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä–æ–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö

```sql
-- –°–æ–∑–¥–∞–π—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∑–∞–¥–∞—á—É –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏:
-- 1. –î—É–±–ª–∏–∫–∞—Ç–æ–≤
-- 2. –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (negative prices, future dates)
-- 3. Referential integrity (–≤—Å–µ customer_id —Å—É—â–µ—Å—Ç–≤—É—é—Ç –≤ customers)
```

## –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã

1. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**
   - –û–±—ä—è—Å–Ω–∏—Ç–µ flow –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ —Å–∏—Å—Ç–µ–º—É
   - –ó–∞—á–µ–º –Ω—É–∂–Ω–∞ staging —Ç–∞–±–ª–∏—Ü–∞?
   - –ü–æ—á–µ–º—É –ø—Ä–æ–¥—É–∫—Ç—ã –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –≤ REPLICATED —Ç–∞–±–ª–∏—Ü—É?

2. **–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å:**
   - –ö–∞–∫–∏–µ –º–µ—Ä—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã?
   - –ö–∞–∫ –∑–∞—â–∏—â–µ–Ω –¥–æ—Å—Ç—É–ø –∫ S3?
   - –î–ª—è —á–µ–≥–æ –Ω—É–∂–Ω—ã Security Groups?

3. **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:**
   - –ö–∞–∫ –º–æ–∂–Ω–æ —É—Å–∫–æ—Ä–∏—Ç—å –∑–∞–≥—Ä—É–∑–∫—É –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤?
   - –ó–∞—á–µ–º –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã sales?
   - –ü–æ—á–µ–º—É —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ DISTRIBUTED –¥–ª—è —Ç–∞–±–ª–∏—Ü?

4. **Airflow:**
   - –û–±—ä—è—Å–Ω–∏—Ç–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏ –≤ DAG
   - –ß—Ç–æ –ø—Ä–æ–∏–∑–æ–π–¥–µ—Ç –ø—Ä–∏ –ø–∞–¥–µ–Ω–∏–∏ –∑–∞–¥–∞—á–∏?
   - –ö–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è?

## –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤

```bash
# –í–ù–ò–ú–ê–ù–ò–ï: –£–¥–∞–ª—è–µ—Ç –≤—Å–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã!

# –£–¥–∞–ª–µ–Ω–∏–µ Airflow –∫–ª–∞—Å—Ç–µ—Ä–∞
yc airflow cluster delete etl-airflow-cluster

# –û—á–∏—Å—Ç–∫–∞ S3
aws s3 rm s3://$BUCKET_NAME --recursive \
  --endpoint-url=https://storage.yandexcloud.net

# –£–¥–∞–ª–µ–Ω–∏–µ –±–∞–∫–µ—Ç–∞
yc storage bucket delete --name $BUCKET_NAME

# –£–¥–∞–ª–µ–Ω–∏–µ —Å–µ—Ä–≤–∏—Å–Ω—ã—Ö –∞–∫–∫–∞—É–Ω—Ç–æ–≤
yc iam service-account delete s3-airflow-sa
yc iam service-account delete airflow-sa

# –£–¥–∞–ª–µ–Ω–∏–µ security group
yc vpc security-group delete airflow-sg

# –ù–ï —É–¥–∞–ª—è–π—Ç–µ GreenPlum –µ—Å–ª–∏ –ø–ª–∞–Ω–∏—Ä—É–µ—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä –Ω—ã–µ!
```

## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã

–ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã –≤—ã –¥–æ–ª–∂–Ω—ã:

‚úÖ –ü–æ–Ω–∏–º–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É ETL —Å Airflow  
‚úÖ –£–º–µ—Ç—å —Å–æ–∑–¥–∞–≤–∞—Ç—å Managed Airflow –≤ Yandex Cloud  
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å —Å–µ—Ç–µ–≤—É—é —Å–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å –º–µ–∂–¥—É —Å–µ—Ä–≤–∏—Å–∞–º–∏  
‚úÖ –†–∞–±–æ—Ç–∞—Ç—å —Å Object Storage (S3)  
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å DAG –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ ETL  
‚úÖ –ó–∞–≥—Ä—É–∂–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ (CSV, JSON)  
‚úÖ –ü—Ä–∏–º–µ–Ω—è—Ç—å staging –ø–æ–¥—Ö–æ–¥ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏  
‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ ETL –∑–∞–¥–∞—á  

## –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- [Yandex Managed Airflow](https://cloud.yandex.ru/docs/managed-airflow/)
- [Yandex Object Storage](https://cloud.yandex.ru/docs/storage/)
- [Apache Airflow Docs](https://airflow.apache.org/docs/)
- [GreenPlum Best Practices](https://docs.vmware.com/en/VMware-Greenplum/)

---

**–ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º —Å –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ–º –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã ‚Ññ4!** üéâ

–í—ã –æ—Å–≤–æ–∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ ETL-–ø—Ä–æ—Ü–µ—Å—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Å—Ç–µ–∫–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π Yandex Cloud!
